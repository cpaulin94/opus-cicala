{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0280e968",
   "metadata": {},
   "source": [
    "# Chatbot LLM minimale\n",
    "\n",
    "1. Esporta `LLM_PROVIDER` (`openai`, `anthropic`, `gemini`), `LLM_API_KEY` e facoltativamente `LLM_MODEL`/`LLM_BASE_URL`.\n",
    "2. Installa `requests` (es. `uv add requests`).\n",
    "3. Avvia la cella Python seguente per chattare dal terminale del notebook con temperatura e prompt regolabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def call_llm(prompt: str, system_prompt: str = \"\", temperature: float = 0.7) -> str:\n",
    "    provider = os.getenv(\"LLM_PROVIDER\", \"openai\").lower()\n",
    "    api_key = os.environ[\"LLM_API_KEY\"]\n",
    "    params = None\n",
    "    if provider == \"anthropic\":\n",
    "        url = \"https://api.anthropic.com/v1/messages\"\n",
    "        headers = {\n",
    "            \"x-api-key\": api_key,\n",
    "            \"anthropic-version\": \"2023-06-01\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": os.getenv(\"LLM_MODEL\", \"claude-3-haiku-20240307\"),\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": temperature,\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }],\n",
    "        }\n",
    "        if system_prompt:\n",
    "            payload[\"system\"] = system_prompt\n",
    "    elif provider == \"gemini\":\n",
    "        url = \"https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent\".format(\n",
    "            os.getenv(\"LLM_MODEL\", \"gemini-1.5-flash\")\n",
    "        )\n",
    "        headers = {\"content-type\": \"application/json\"}\n",
    "        params = {\"key\": api_key}\n",
    "        payload = {\n",
    "            \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "            \"generationConfig\": {\"temperature\": temperature},\n",
    "        }\n",
    "        if system_prompt:\n",
    "            payload[\"system_instruction\"] = {\"parts\": [{\"text\": system_prompt}]}\n",
    "    else:\n",
    "        url = os.getenv(\"LLM_BASE_URL\", \"https://api.openai.com/v1/chat/completions\")\n",
    "        headers = {\n",
    "            \"authorization\": f\"Bearer {api_key}\",\n",
    "            \"content-type\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": os.getenv(\"LLM_MODEL\", \"gpt-3.5-turbo\"),\n",
    "            \"temperature\": temperature,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt or \"Rispondi in modo utile.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    response = requests.post(url, headers=headers, params=params, json=payload, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    if provider == \"anthropic\":\n",
    "        return \"\".join(part[\"text\"] for part in data[\"content\"] if part.get(\"type\") == \"text\").strip()\n",
    "    if provider == \"gemini\":\n",
    "        return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].strip()\n",
    "    return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "def chat_cli() -> None:\n",
    "    system_prompt = input(\"System prompt (lascia vuoto per nessuno): \").strip()\n",
    "    temp_raw = input(\"Temperatura [default 0.7]: \").strip()\n",
    "    temperature = float(temp_raw) if temp_raw else 0.7\n",
    "    print(\"Scrivi 'exit' o 'quit' per uscire.\")\n",
    "    while True:\n",
    "        user_message = input(\"Tu: \").strip()\n",
    "        if user_message.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        try:\n",
    "            reply = call_llm(user_message, system_prompt, temperature)\n",
    "        except Exception as exc:\n",
    "            print(f\"Errore: {exc}\")\n",
    "            continue\n",
    "        print(f\"Bot: {reply}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_cli()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
